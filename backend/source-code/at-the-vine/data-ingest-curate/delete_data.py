""" 
Jaye Hicks 2020

Obligatory legal disclaimer:
  You are free to use this source code (this file and all other files 
  referenced in this file) "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER 
  EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. 
  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THIS SOURCE CODE
  IS WITH YOU.  SHOULD THE SOURCE CODE PROVE DEFECTIVE, YOU ASSUME THE
  COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. See the GNU 
  GENERAL PUBLIC LICENSE Version 3, 29 June 2007 for more details.
  
This module serves as an AWS Lambda function intended to be invoked (on
a scheduled, regular basis) by a CloudWatch Events Rule in order to 
delete raw sensor data housed in a DynamoDB table table_name_1.  
Specifically, data that has aged beyond the defined age threhold will
be deleted. 

Lonesome Vine Vineyard operates a maximum of 21 sensors stations.  On a 
monthly basis the maximum number of DynamoDB Items that could be added
to the DynamoDB table table_name_1 is 15,624 (21 stations x 24 sensor
station reports per station per day x 31 days).  An upper bound estimate
of 70 bytes per single sensor station report yields 1,093,680 bytes of 
total raw sensor data per month.

Empirical observation of this module running on a laptop, working with
max volumes of raw sensor data, yielded run times of between 5 and 20
seconds.  This was over a 50 MBs Internet connection (i.e., north
Texas to North Virginia).  Throttling was when the DynamoDB table was
configured at the default settings of 5 read capacity units and 5 write 
capacity units.  To address the throttling issue the table was modified 
from default capacity settings to 'on-demand' read/write capacity.

The DynamoDB table contains raw vineyard sensor station data across all 
21 vineyard sensor stations.  A separate Python module is responsible 
for archiving off this raw data (i.e., before this module deletes it) 
by grouping all data for all sensors that belong to a single calendar 
month and saving that data in a CSV file that is stored in an S3 bucket.
A check is made to ensure that data has been archived prior to deleting
the data.

Informational and error messages generated by this module, and other 
Python modules that work with this raw vineyard sensor data (e.g., 
ingestion, life cycle management, freeze/battery/absence guard), are 
stored in two DynamoDB tables.  Messages stored in the table for errors 
will not automatically expire; requires manual data lifecycle management.
Messages stored in the the table for informational messages is inserted 
with a TTL value and these items are automatically purged after reaching 
a certain age.

Usage: 
  A CloudWatch Events Rule invokes, on an regularly scheduled basis, this
  AWS Lambda function by calling the function lambda_handler and passing
  to it the input parameters 'event' and 'context.'

Dependencies:
  import boto3
  import time
  from boto3.dynamodb.conditions import Key
  from botocore.exceptions import ClientError
  from dateutil.relativedelta import * 
  from datetime import datetime, timedelta
"""
import boto3
import time
from boto3.dynamodb.conditions import Key
from botocore.exceptions import ClientError
from dateutil.relativedelta import * 
from datetime import datetime, timedelta

ARCHIVAL_BUCKET_NAME           = 'bucket_name'
RAW_DATA_TABLE                 = 'table_name_1'
MONTH_PRIOR_TO_OLDEST_RAW_DATA = '2020-01'
#MONTH_PRIOR_TO_OLDEST_RAW_DATA = '2019-07'  #debug setting only         
AGE_LIMIT                      = 3 #delete raw sensor data 3+ months old

JOB                 = 'delete_data'
ERROR               = 1
ALARM               = 2
INFO                = 3
WARN                = 4
error_messages      = {}
info_messages       = {}


def _write_logs_to_databases():
  """
  As part of the backend management / processing of vineyard sensor 
  stations raw data, messages are recorded to DynamoDB tables by the
  various Python jobs that ingest and curate the data.  Informational
  message have a TTL, error messages do not and require manual curation.
  Due to low volume, a batch writer to insert messages not needed.
  """
  try:
    dynamo_db_access = boto3.resource('dynamodb')
    if(error_messages):
      try:
        table = dynamo_db_access.Table('table_name_2')
        for key, value in error_messages.items():
          table.put_item(Item={'date' : value['date'], 
                               'stamp_job' : key,
                               'message': value['message']})
      except:
        pass  #where do you log when logging doesn't work?
    if(info_messages):
      try:
        table = dynamo_db_access.Table('table_name_3')
        for key, value in info_messages.items():
          table.put_item(Item={'date': value['date'],
                               'stamp_job' : key, 
                               'message': value['message'], 
                               'expiry' : value['expiry']})
      except:
        pass #where do you log when logging doesn't work?
  except:
    pass #where do you log when logging doesn't work?
  
  
def _log_message(position, type, message, exception):
  """
  As part of the backend management / processing of vineyard sensor 
  stations raw data, messages are recorded to DynamoDB tables.  As
  the epoch timestamp used is in seconds, vs milliseconds, the 1
  second delay is used to avoid sort key clashes.  Wont be a performance
  issue as this funciton is called infrequently.
  
  Note: 2038 epoch time roll over issue
  """    
  time.sleep(1)
  now = datetime.now()
  timestamp = int(now.timestamp()) #converts millisecons to seconds
  cst = now - timedelta(hours=6)   #UTC to US Central Std
  date = str(cst.year) + '-' + str(cst.month).zfill(2) + '-' 
  date += str(cst.day).zfill(2)
  stamp_job = str(timestamp) + '+' + JOB
  prefix = ['','ERROR: (','ALARM: (','INFO: (','WARN: (']
    
  log_message = prefix[type] + position + ') ' + message + ' '  
  if(exception):
    try:
      if(exception.response['Error']['Message']):
        log_message += exception.response['Error']['Message']
      else:
        log_message += str(exception)
    except:
      log_message += str(exception)
  
  if((type == ERROR) or (type == ALARM)):
    error_messages[stamp_job] = {'date' : date, 'message' : log_message}
  else:
    expiration = timestamp + 5184000   #DynamoDB autodelete in 2 months
    info_messages[stamp_job] = {'date' : date, 'message' : log_message,
                                'expiry' : expiration}
                                
 
def _archive_file_exists(date):
  """
  Determine if the arhival file for all of the raw sensor data for a
  'year-mo' exists in the archival bucket.
  """
  exists = False
  archive_file = date + '-VineyardStations.csv'
    
  try:
    s3_access_client = boto3.client('s3')
    s3_access_client.head_bucket(Bucket=ARCHIVAL_BUCKET_NAME)
    try:
      s3_access_resource = boto3.resource('s3')
      s3_access_resource.Object(ARCHIVAL_BUCKET_NAME, archive_file).load()
      exists = True
    except Exception as e: 
        pass # simply allow False to be returned
            
  except ClientError as e:
    error_code = int(e.response['Error']['Code'])
    if(error_code == 404):
      _log_message('11', ERROR, 'The archival bucket "' + 
                   ARCHIVAL_BUCKET_NAME + '" does not exist.', '')  
    else:
      _log_message('12', ERROR, '', e)
  except Exception as e: 
    _log_message('13', ERROR, '', e)  
   
  return(exists)
 
def delete_data(event, context):
  """
  Raw vineyard sensor data for all sensor stations is stored in a single
  DynamoDB table.  Generate a list of all possible partition keys for 
  Items in the table (e.g., ['2020-01', '2020-02', ...]) for data that
  is old enough to have been archived.  Step through this list and
  delete any raw sensor data records from the table that has a partition
  key past the threshold (e.g., delete all records older than 6 months).
  
  Args (supplied by AWS Lambda service)
    event: information about who/what invoked the Lambda function
    context: information about the Lambda function's runtime environment
  """
  #necessary as Lambda service caches globals between function invocations
  global error_messages
  error_messages  = {}
  global info_messages
  info_messages  = {}
  
  delete_dates = _all_possible_dates_to_delete()

  try:
    dynamo_db_access = boto3.resource('dynamodb')
    table = dynamo_db_access.Table(RAW_DATA_TABLE)
    for date in delete_dates:
      try:
        response = table.query(KeyConditionExpression=Key('date').eq(date))
        if(response['Items']):
          if(_archive_file_exists(date)):
            if(not _delete_items(date,response)):
              break
          else:
            _log_message('1', WARN, 'Stopped deletion of raw sensor data ' +
              'records belonging to "' + date + '" because they have not yet' +
              ' been archived.', '')            
      except Exception as e:
        _log_message('2', ERROR, '', e)
  except Exception as e:
    _log_message('3', ERROR, '', e) 
    
  if(error_messages):
    _log_message('4', WARN, 
      'Delete Data process did not complete successfully.', '') 
  else:
    _log_message('5', INFO, 'Delete Data process completed successfully.', '') 
    
  _write_logs_to_databases()
 

def _delete_items(date, response):
  """
  Delete all Items from Table with parition key == to parameter 'date'
  For example, delete all Items from DynamoDB table x
  where the Item's attribute 'date' (i.e., the partition key) is ==
  to a specific 'year-mo' (e.g., '2020-08')
  """
  result = True
  page_to_process = True
  items_deleted = 0
          
  try:
    dynamo_db_access = boto3.resource('dynamodb')
    table = dynamo_db_access.Table(RAW_DATA_TABLE)
    with table.batch_writer() as batch:
      while(page_to_process):
        for item in response['Items']:
          try:
            batch.delete_item(Key={'date': date,
                                   'day_ts_loc': item['day_ts_loc']})
            items_deleted += 1
          except Exception as e:
            result = False
            _log_message('6', ERROR, '', e)   
            break
        #if response has multiple pages, process them all
        if('LastEvaluatedKey' in response):
          try:
            response = table.query(
              KeyConditionExpression=Key('date').eq(date),
              ExclusiveStartKey=response['LastEvaluatedKey'])            
          except Exception as e:
            result = False
            page_to_process = False
            _log_message('7', ERROR, '', e)
            break
        else:
          page_to_process = False
   
  except Exception as e:
    result = False
    _log_message('8', ERROR, '', e)
   
  if(result):
    _log_message('9', INFO, 'Deleted all raw sensor data records for: ' + date
                 + ' (' + str(items_deleted) + ' sensor records deleted).', '')
  return(result)
 
  
def _all_possible_dates_to_delete():
  """
  Create a list of year-month values (e.g., ['2020-01', '2019-12']) for 
  all months that are past age limit.  Create last that goes back in
  time up until the oldest possible raw data sensor records.  
  """
  delete_dates = []
  CURRENT_DATE_TIME = datetime.now()
  backward_month_counter = AGE_LIMIT

  while(backward_month_counter):
    prior_date_time = (CURRENT_DATE_TIME - 
      relativedelta(months=backward_month_counter))
    year_month = (str(prior_date_time.year) + '-' + 
      str(prior_date_time.month).zfill(2))
    if(year_month == MONTH_PRIOR_TO_OLDEST_RAW_DATA):
      backward_month_counter = 0
      break
    else:                        
      delete_dates.append(year_month)
      backward_month_counter += 1
  return(delete_dates)