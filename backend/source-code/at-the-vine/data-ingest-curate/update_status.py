""" 
Jaye Hicks 2020

Obligatory legal disclaimer:
  You are free to use this source code (this file and all other files 
  referenced in this file) "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER 
  EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. 
  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THIS SOURCE CODE
  IS WITH YOU.  SHOULD THE SOURCE CODE PROVE DEFECTIVE, YOU ASSUME THE
  COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. See the GNU 
  GENERAL PUBLIC LICENSE Version 3, 29 June 2007 for more details.

This module serves as an AWS Lambda function intended to be invoked on
an adhoc basis when an update occurs to an IoT Core shadow document.
The update to IoT Core shadow documents typically occurs on a regularly
scheduled basis; updates occur as a result of the regular execution 
of the job pull_data.

A Python job, named pull_data, is regularly executed to access all 
recently arrived sensor data from any of the 21 vineayrd sensor
stations.  It retrieves this data from an Internet endpoint (i.e., 
www.thingspeak.com).  After collecting the sensor data records, 
pull_data cycles through records updating the IoT Core shadow document
that corresponds to the sensor record.  A dedicated IoT Core shadow 
document is assigned to each of the 21 sensor stations.  Incoming 
sensor data needs to route to three different AWS destinations: two 
DyanmoDB tables and to one of 21 CloudWatch custom metrics.  AWS 
service-to-service integrations handle two of these destinations but 
a Lambda function (i.e. this module) is required to handle the 
'existing Item update' integration with the DynamoDB table
table_name.
 
Informational and error messages generated by this module, and other 
Python modules that work with this raw vineyard sensor data (e.g., 
ingestion, life cycle management, freeze/battery/absence guard), are 
stored in two DynamoDB tables.  Messages stored in the table for errors, 
another_table_name, will not automatically expire and require 
manual data lifecycle management. Messages stored in the the table for 
informational missives, VinStationsBackEndInfo, are inserted with a 
TTL value and are therefore automatically purged after reaching a 
certain age.

Usage: 
  Invoked via IoT Core -> Lambda service integration.  Values to use for
  updating the DynamoDB table passed via the default Lambda service
  parameters of 'event' and 'context.'

Dependencies:
  import boto3
  import time
  from boto3.dynamodb.conditions import Key
  from datetime import datetime, timedelta
"""
import boto3
import time
from boto3.dynamodb.conditions import Key
from datetime import datetime, timedelta

STATUS_TABLE = 'table_name'
LOCATIONS    = ['A02N', 'A08M', 'A02S', 'B02N', 'B08M', 'B02S', 'C02N',
                'C08M', 'C02S', 'D02N', 'D08M', 'D02S', 'E02N', 'E08M', 
                'E02S', 'F02N', 'F08M', 'F02S', 'G02N', 'G08M', 'G02S']

JOB                 = 'update_status'  
ERROR               = 1
ALARM               = 2
INFO                = 3
WARN                = 4
error_messages      = {}
info_messages       = {}
     
     
def _write_logs_to_databases():
  """
  As part of the backend management / processing of vineyard sensor 
  stations raw data, messages are recorded to DynamoDB tables by the
  various Python jobs that ingest and curate the data.  Informational
  message have a TTL, error messages do not and require manual curation.
  Due to low volume, a batch writer to insert messages not needed.
  """
  try:
    dynamo_db_access = boto3.resource('dynamodb')
    if(error_messages):
      try:
        table = dynamo_db_access.Table('table_name_1')
        for key, value in error_messages.items():
          table.put_item(Item={'date' : value['date'], 
                               'stamp_job' : key,
                               'message': value['message']})
      except:
        pass  #where do you log when logging doesn't work?
    if(info_messages):
      try:
        table = dynamo_db_access.Table('table_name_2')
        for key, value in info_messages.items():
          table.put_item(Item={'date': value['date'],
                               'stamp_job' : key, 
                               'message': value['message'], 
                               'expiry' : value['expiry']})
      except:
        pass #where do you log when logging doesn't work?
  except:
    pass #where do you log when logging doesn't work?
  
  
def _log_message(position, type, message, exception):
  """
  As part of the backend management / processing of vineyard sensor 
  stations raw data, messages are recorded to DynamoDB tables.  As
  the epoch timestamp used is in seconds, vs milliseconds, the 1
  second delay is used to avoid sort key clashes.  Wont be a performance
  issue as this funciton is called infrequently.
  
  Note: 2038 epoch time roll over issue
  """    
  time.sleep(1)
  now = datetime.now()
  timestamp = int(now.timestamp()) #converts millisecons to seconds
  cst = now - timedelta(hours=6)   #UTC to US Central Std
  date = str(cst.year) + '-' + str(cst.month).zfill(2) + '-' 
  date += str(cst.day).zfill(2)
  stamp_job = str(timestamp) + '+' + JOB
  prefix = ['','ERROR: (','ALARM: (','INFO: (','WARN: (']
    
  log_message = prefix[type] + position + ') ' + message + ' '  
  if(exception):
    try:
      if(exception.response['Error']['Message']):
        log_message += exception.response['Error']['Message']
      else:
        log_message += str(exception)
    except:
      log_message += str(exception)
  
  if((type == ERROR) or (type == ALARM)):
    error_messages[stamp_job] = {'date' : date, 'message' : log_message}
  else:
    expiration = timestamp + 5184000   #DynamoDB autodelete in 2 months
    info_messages[stamp_job] = {'date' : date, 'message' : log_message,
                                'expiry' : expiration}

 
def update_status(event, context):
  """
  The DynamoDB table dedicated for maintaining the last reported values
  for vineyard sensor stations, table_name, contains 21 Items;
  one for each of the 21 stations.  There is a dedicated IoT Core
  shadow document for each of the 21 stations.  At any given point in
  time table_name_again will contain the most recent sensor data
  readings across all 21 stations.
  
  IoT Core to DynamoDB service-to-service integration only supports 
  the insertion of new data (i.e., new Items).  That service
  integration is used for the DynamoDB table table_name.  However,
  this module is required in order to support the updating of an 
  existing DynamoDB record/Item.  The IoT Core to DynamoDB 
  service-to-service integration calls this Lambda function which in 
  turn carries out the update of the Item.

  Args (supplied by AWS Lambda service)
    event: information about who/what invoked the Lambda function
    context: information about the Lambda function's runtime environment
  """   
  #necessary as Lambda service caches globals between function invocations
  global error_messages
  error_messages  = {}
  global info_messages
  info_messages  = {}

  if(event):
    parts = event['day_ts_loc'].split('+')
    location = parts[2].upper()
    if(location in LOCATIONS):
      try:
        dynamo_db_access = boto3.resource('dynamodb')
        table = dynamo_db_access.Table(STATUS_TABLE)
          
        try:
          update_expression = 'set battery = :b, tstamp = :ti, '
          update_expression += 'temperature = :te, sms1 = :s1, sms2 = :s2, '
          update_expression += 'sms3 = :s3'
          table.update_item(
            Key={'location': location},
                 UpdateExpression=update_expression,
                 ExpressionAttributeValues={':b' : event['battery'],
                                            ':ti' : parts[1],
                                            ':te' : event['temp'],
                                            ':s1' : event['sms1'],
                                            ':s2' : event['sms2'],
                                            ':s3' : event['sms3']}) 
        except Exception as e:
          _log_message('1', ERROR, '', e)
      except Exception as e:
        _log_message('2', ERROR, '', e)
    else:
      _log_message('3', ERROR, 'Raw sensor data update record received '
                   + 'with invalid station location: ' + parts[2], '')
  else:
    _log_message('4', ERROR, 'Empty parameter sent to Lambda function.', '')
  
  if(error_messages):
    _log_message('5', WARN, 
      'Update Status process did not complete successfully.', '')
  else:
    _log_message('6', INFO, 
      'Update Status process completed successfully.', '')
  
  _write_logs_to_databases()