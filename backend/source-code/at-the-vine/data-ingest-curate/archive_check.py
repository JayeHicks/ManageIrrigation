""" 
ver: 2021-03-18
This module serves as an AWS Lambda function intended to be invoked on
a regular scheduled basis by a CloudWatch Events rule in order to 
detect missing archived vineyard sensor station files as well as
archived vineyard sensor station files that appear unusually small.
This will be accomplished by accessed the archival S3 bucket, 
searching for the existance of files, and checking the size of any
archival files located.  archive-check functionality relies on
solution-wide adherance to a strict naming standard for the CSV 
archival files.  These archival files contain raw vineyard sensor 
data, across all 21 stations, bundled up in calendar monthly increments
and reside in the S3 bucket named 'my-archive-bucket.'  
This module will not send alerts; it will log a WARN informational
logging message if it unconvers anything suspect.

It is recommended that this Python module be run every 2 - 4 weeks.

Informational and error messages generated by this module, and other 
Python modules that work with this raw vineyard sensor data (e.g., 
ingestion, life cycle management, freeze/battery/absence guard), are 
stored in two DynamoDB tables.  Messages stored in the table for errors 
will not automatically expire; requires manual data lifecycle 
management. Messages stored in the the table for informational messages
are inserted with a TTL value and these items are automatically purged 
after reaching a certain age.

Usage: 
  A CloudWatch Events Rule invokes, on an regularly scheduled basis, 
  this AWS Lambda function by calling the function lambda_handler and 
  passing to it the input parameters 'event' and 'context.'

Dependencies:
  import boto3
  import time
  from botocore.exceptions import ClientError
  from boto3.dynamodb.conditions import Key
  from dateutil.relativedelta import * 
  from datetime import datetime, timedelta
"""
import boto3
import time
from botocore.exceptions import ClientError
from boto3.dynamodb.conditions import Key
from dateutil.relativedelta import * 
from datetime import datetime, timedelta

ARCHIVAL_BUCKET_NAME       = 'my-archive-bucket'
CONTROL_TABLE              = 'my-control-table'
STATIONS_PER_ARCHIVE_TABLE = 'my-archive-table'
CONTROL_ITEM_SELECT        = '2'
MONTHS_BACK_LIMIT          = 12 #how months back to go back
OLDEST_ARCHIVE             = '2020-03'

JOB                 = 'archive_check'
ERROR               = 1
ALARM               = 2
INFO                = 3
WARN                = 4
error_messages      = {}
info_messages       = {}
     
     
def _write_logs_to_databases():
  """
  As part of the backend management / processing of vineyard sensor 
  stations raw data, messages are recorded to DynamoDB tables by the
  various Python jobs that ingest and curate the data.  Informational
  message have a TTL, error messages do not and require manual curation.
  Due to low volume, a batch writer to insert messages not needed.
  """
  try:
    dynamo_db_access = boto3.resource('dynamodb')
    if(error_messages):
      try:
        table = dynamo_db_access.Table('my-issues-table')
        for key, value in error_messages.items():
          table.put_item(Item={'date' : value['date'], 
                               'stamp_job' : key,
                               'message': value['message']})
      except:
        pass  #where do you log when logging doesn't work?
    if(info_messages):
      try:
        table = dynamo_db_access.Table('my-info-table')
        for key, value in info_messages.items():
          table.put_item(Item={'date': value['date'],
                               'stamp_job' : key, 
                               'message': value['message'], 
                               'expiry' : value['expiry']})
      except:
        pass #where do you log when logging doesn't work?
  except:
    pass #where do you log when logging doesn't work?
  
  
def _log_message(position, type, message, exception):
  """
  As part of the backend management / processing of vineyard sensor 
  stations raw data, messages are recorded to DynamoDB tables.  As
  the epoch timestamp used is in seconds, vs milliseconds, the 1
  second delay is used to avoid sort key clashes.  Wont be a performance
  issue as this funciton is called infrequently.
  
  Note: 2038 epoch time roll over issue
  """    
  time.sleep(1)
  now = datetime.now()
  timestamp = int(now.timestamp()) #converts millisecons to seconds
  cst = now - timedelta(hours=6)   #UTC to US Central Std
  date = str(cst.year) + '-' + str(cst.month).zfill(2) + '-' 
  date += str(cst.day).zfill(2)
  stamp_job = str(timestamp) + '+' + JOB
  prefix = ['','ERROR: (','ALARM: (','INFO: (','WARN: (']
    
  log_message = prefix[type] + position + ') ' + message + ' '  
  if(exception):
    try:
      if(exception.response['Error']['Message']):
        log_message += exception.response['Error']['Message']
      else:
        log_message += str(exception)
    except:
      log_message += str(exception)
  
  if((type == ERROR) or (type == ALARM)):
    error_messages[stamp_job] = {'date' : date, 'message' : log_message}
  else:
    expiration = timestamp + 5184000   #DynamoDB autodelete in 2 months
    info_messages[stamp_job] = {'date' : date, 'message' : log_message,
                                'expiry' : expiration}
 
 
def _archive_file_is_small(file, size, threshold):
  """
  Retrieve the number of sensor stations that contributed sensor data
  to this archive file.  If the number is not available assume 1
  sensor contributed to archive file.  The minimum amount of sensor 
  data that a single sensor creates in a month is stored in database
  and passed into this function.  Example:  37,800 bytes = 75% of 
  (30 days x 24 hours x 70 bytes)
  """
  result = False
    
  try:
    dynamo_db_access = boto3.resource('dynamodb')
    table = dynamo_db_access.Table(STATIONS_PER_ARCHIVE_TABLE)
    try:
      response = table.query(
        KeyConditionExpression=Key('date').eq(file[:7]))
      if(response['Items']):
        number_of_stations = int(response['Items'][0]['count'])
        if(size < (number_of_stations * threshold)):
          result = True        
      elif(size < threshold):
        result = True                
    except Exception as e:
      _log_message('1', ERROR, '', e)    
  except Exception as e:
    _log_message('2', ERROR, '', e)
  return(result)

 
def archive_check(event, context):
  """
  Look across into the S3 archival bucket and generate a list of all
  missing archive files and a list of all archive files that appear 
  unusually small.  Log WARN messages if anything found.
  
  Note: the non existance of expected archive file is detected by 
  catching an exception (which does not represent a processing error)  

  Args (supplied by AWS Lambda service)
    event: information about who/what invoked the Lambda function
    context: information about the Lambda function's runtime environment
  """
  warning_prefix1 = 'The following archive files are missing: '
  warning_prefix2 = 'The following archive files are unusually small: '
  
  #necessary as Lambda service caches globals between function invocations
  global error_messages
  error_messages  = {}
  global info_messages
  info_messages  = {}
      
  try:
    s3_access_client = boto3.client('s3')
    s3_access_client.head_bucket(Bucket=ARCHIVAL_BUCKET_NAME)
    try:
      s3_access_resource = boto3.resource('s3')
      try:
        dynamo_db_access = boto3.resource('dynamodb')
        table = dynamo_db_access.Table(CONTROL_TABLE)
        try:
          response = table.query(
            KeyConditionExpression=Key('control_id').eq(CONTROL_ITEM_SELECT))
          if(response['Items']):
            ARCH_FILE_SIZE_THREHSOLD = int(
                                         response['Items'][0]['min_file_size'])
            file_names = _all_possible_archive_file_names()
            missing_files = []
            small_files = []
            for a_file in file_names:
              try:
                s3_access_resource.Object(ARCHIVAL_BUCKET_NAME, a_file).load()
                object_summary = s3_access_resource.ObjectSummary(
                                                  ARCHIVAL_BUCKET_NAME, a_file)
                if(_archive_file_is_small(a_file, object_summary.size, 
                                          ARCH_FILE_SIZE_THREHSOLD)):
                  small_files.append(a_file)
              except ClientError as e: 
                error_code = int(e.response['Error']['Code'])
        
                if(error_code == 404):    #not an error; detects missing files
                  missing_files.append(a_file)
                else:                     #a processing error detected
                  _log_message('3', ERROR, '', e)  
                  break
              except Exception as e:      #caught non ClientError exception
                _log_message('12', ERROR, '', e)  
                break
                
            if(missing_files):
              message = warning_prefix1
              for file in missing_files:
                message += file + ', '
              _log_message('4', WARN, message[:-2],'')

            if(small_files):
              message = warning_prefix2
              for file in small_files:
                message += file + ', '
              _log_message('5', WARN, message[:-2],'')     
                 
          else:
            _log_message('6', ERROR, 'Could not read control Item ' + 
              CONTROL_ITEM_SELECT + ' from DynamoDB control table.','')
        except Exception as e:
          _log_message('7', ERROR, '', e) 
      except Exception as e:
        _log_message('8', ERROR, '', e)               
    except Exception as e:
      _log_message('9', ERROR, '', e)             
  except ClientError as e:
    error_code = int(e.response['Error']['Code'])
    if(error_code == 404):
      _log_message('10', ERROR, 'The archival bucket "' + 
        ARCHIVAL_BUCKET_NAME + '" does not exist.', '')
    else:
      _log_message('11', ERROR, '', e)               
  except Exception as e:                  #caught non ClientError exception
    _log_message('13', ERROR, '', e)               
  
  if(error_messages):
    _log_message('14', WARN, 'Archive Check process did not complete ' +
      'successfully.', '')
  else:
    _log_message('15', INFO, 
      'Archive Check process completed successfully.', '')
  _write_logs_to_databases()  

def _all_possible_archive_file_names():
  """
  Create a list of all object names (i.e., files containing archived raw
  sensor data) that should exist in the S3 archive bucket named
  'my-archive-bucket'.  Archive files created using strict
  naming conventions.  The '>1' in while statement keeps the current
  an previous months from being included in the list.
  """
  file_names = []
  CURRENT_DATE_TIME = datetime.now()
  month_counter = MONTHS_BACK_LIMIT
  
  while(month_counter > 1):
    prior_date_time = (CURRENT_DATE_TIME - relativedelta(months=month_counter))
    year_month = (str(prior_date_time.year) + '-' + 
      str(prior_date_time.month).zfill(2))
    if(year_month >= OLDEST_ARCHIVE):      
      file_names.append(year_month +  '-VineyardStations.csv')
    month_counter -= 1
  return(file_names)